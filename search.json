[
  {
    "objectID": "pages/02 Reading from file.html",
    "href": "pages/02 Reading from file.html",
    "title": "Reading from file",
    "section": "",
    "text": "One of the most common situations is that you have a data file containing the data you want to read. Perhaps this is data you’ve produced yourself or maybe it’s from a colleague or perhaps from an online source. In an ideal world the file will be perfectly formatted and will be trivial to import into pandas but since this is so often not the case, pandas provides a number of features to make your life easier.\nFull information on reading and writing is available in the pandas manual on IO tools but first it’s worth noting the common formats that pandas can work with: - Comma separated tables (or tab-separated or space-separated etc.) - Excel spreadsheets - HDF5 files - SQL databases\nFor this course we will focus on plain-text CSV files as they are perhaps the most common format. It’s also common to be provided with data in an Excel format and Pandas provides all the tools you need to extract the data out of Excel and analyse it in Python.\n\nReading our first file\nYou can get access to Pandas by importing the pandas module. By convention, it is imported as pd:\n\nimport pandas as pd\n\nWe can use the pandas function read_csv() to read the file and convert it to a DataFrame. Full documentation for this function can be found in the manual.\nThe first argument to the function is called filepath_or_buffer, the documentation for which begins:\n\nAny valid string path is acceptable. The string could be a URL…\n\nThis means that we can take a URL and pass it directly (or via a variable) to the function. For example, here is a file I have prepared for you:\n#| tags: []\nrain = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/rain.csv\")\nThis gives us the data from the file as a type of object called a DataFrame. This is the core of Pandas and we will be exploring many of the things that it can do throughout this course.\nWe can get Jupyter to display the data by putting the variable name in a cell by itself:\n\nrain\n\n\n\n\n\n\n\n\nCardiff\nStornoway\nOxford\nArmagh\n\n\n\n\n1853\nNaN\nNaN\n57.7\n53.0\n\n\n1854\nNaN\nNaN\n37.5\n69.8\n\n\n1855\nNaN\nNaN\n53.4\n50.2\n\n\n1856\nNaN\nNaN\n57.2\n55.0\n\n\n1857\nNaN\nNaN\n61.3\n64.6\n\n\n...\n...\n...\n...\n...\n\n\n2016\n99.3\n100.0\n54.8\n61.4\n\n\n2017\n85.0\n103.1\n48.1\n60.7\n\n\n2018\n99.3\n96.8\n48.9\n67.6\n\n\n2019\n119.0\n105.6\n60.5\n72.7\n\n\n2020\n117.6\n121.1\n64.2\n71.3\n\n\n\n\n168 rows × 4 columns\n\n\n\nSo a DataFrame is a table of data, it has columns and rows. In this particular case, the data are the total monthly rainfall (in mm), averaged over each year for each of four cities.\nWe can see there are a few key parts of the output:\n\nDown the left-hand side in bold is the index. These can be thought of as being like row numbers, but can be more informational. In this case they are the year that the data refers to.\nAlong the top are the column names. When we want to refer to a particular column in our DataFrame, we will use these names.\nThe actual data is then arrayed in the middle of the table. Mostly these are data that we care about, but you will also see some NaNs in there as well. This is how Pandas represents missing data, in this case years for which there are no measurements.\n\n\n\nDealing with messy data\nNow let’s move on to how you can deal with the kind of data you’re likely to come across in the real world.\nImagine we have a CSV (comma-separated values) file. The example we will use today is available at city_pop.csv. If you were to open that file then you would see:\nThis is an example CSV file\nThe text at the top here is not part of the data but instead is here\nto describe the file. You'll see this quite often in real-world data.\nA -1 signifies a missing value.\n\nyear;London;Paris;Rome\n2001;7.322;-1;2.547\n2006;7.652;2.18;2.627\n2008;;2.211;2.72\n2009;-1;2.234;2.734\n2011;8.174;2.25;2.76\n2012;8.293;2.244;2.627\n2015;8.615;2.21;\n2019;;;\nThis file has some issues that read_csv will not be able to automatically deal with but let’s start by trying to read it in directly:\ncity_pop_file = \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/city_pop.csv\"\npd.read_csv(city_pop_file)\n\n\n\n\n\n\n\n\n\nThis is an example CSV file\n\n\n\n\n0\nThe text at the top here is not part of the da...\n\n\n1\nto describe the file. You'll see this quite of...\n\n\n2\nA -1 signifies a missing value.\n\n\n3\nyear;London;Paris;Rome\n\n\n4\n2001;7.322;-1;2.547\n\n\n5\n2006;7.652;2.18;2.627\n\n\n6\n2008;;2.211;2.72\n\n\n7\n2009;-1;2.234;2.734\n\n\n8\n2011;8.174;2.25;2.76\n\n\n9\n2012;8.293;2.244;2.627\n\n\n10\n2015;8.615;2.21;\n\n\n11\n2019;;;\n\n\n\n\n\n\n\nWe can see that by default it’s done a fairly bad job of parsing the file (this is mostly because I’ve constructed the city_pop.csv file to be as obtuse as possible). It’s making a lot of assumptions about the structure of the file but in general it’s taking quite a naïve approach.\n\nSkipping the header\nThe first thing we notice is that it’s treating the text at the top of the file as though it’s data. Checking the documentation we see that the simplest way to solve this is to use the skiprows argument to the function to which we give an integer giving the number of rows to skip (also note that I’ve changed to put one argument per line for readability and that the comma at the end is optional but for consistency):\n\npd.read_csv(\n    city_pop_file,\n    skiprows=5,  # Add this\n)\n\n\n\n\n\n\n\n\nyear;London;Paris;Rome\n\n\n\n\n0\n2001;7.322;-1;2.547\n\n\n1\n2006;7.652;2.18;2.627\n\n\n2\n2008;;2.211;2.72\n\n\n3\n2009;-1;2.234;2.734\n\n\n4\n2011;8.174;2.25;2.76\n\n\n5\n2012;8.293;2.244;2.627\n\n\n6\n2015;8.615;2.21;\n\n\n7\n2019;;;\n\n\n\n\n\n\n\n\n\nSpecifying the separator\nThe next most obvious problem is that it is not separating the columns at all. This is controlled by the sep argument which is set to ',' by default (hence comma separated values). We can simply set it to the appropriate semi-colon:\n\npd.read_csv(\n    city_pop_file,\n    skiprows=5,\n    sep=\";\",  # Add this\n)\n\n\n\n\n\n\n\n\nyear\nLondon\nParis\nRome\n\n\n\n\n0\n2001\n7.322\n-1.000\n2.547\n\n\n1\n2006\n7.652\n2.180\n2.627\n\n\n2\n2008\nNaN\n2.211\n2.720\n\n\n3\n2009\n-1.000\n2.234\n2.734\n\n\n4\n2011\n8.174\n2.250\n2.760\n\n\n5\n2012\n8.293\n2.244\n2.627\n\n\n6\n2015\n8.615\n2.210\nNaN\n\n\n7\n2019\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNow it’s actually starting to look like a real table of data.\n\n\nIdentifying missing data\nReading the descriptive header of our data file we see that a value of -1 signifies a missing reading so we should mark those too. This can be done after the fact but it is simplest to do it at file read-time using the na_values argument:\n\npd.read_csv(\n    city_pop_file,\n    skiprows=5,\n    sep=\";\",\n    na_values=\"-1\",  # Add this\n)\n\n\n\n\n\n\n\n\nyear\nLondon\nParis\nRome\n\n\n\n\n0\n2001\n7.322\nNaN\n2.547\n\n\n1\n2006\n7.652\n2.180\n2.627\n\n\n2\n2008\nNaN\n2.211\n2.720\n\n\n3\n2009\nNaN\n2.234\n2.734\n\n\n4\n2011\n8.174\n2.250\n2.760\n\n\n5\n2012\n8.293\n2.244\n2.627\n\n\n6\n2015\n8.615\n2.210\nNaN\n\n\n7\n2019\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nSetting the index\nThe last this we want to do is use the year column as the index for the DataFrame. This can be done by passing the name of the column to the index_col argument:\n\ncensus = pd.read_csv(\n    city_pop_file,\n    skiprows=5,\n    sep=\";\",\n    na_values=\"-1\",\n    index_col=\"year\",  # Add this\n)\ncensus\n\n\n\n\n\n\n\n\nLondon\nParis\nRome\n\n\nyear\n\n\n\n\n\n\n\n2001\n7.322\nNaN\n2.547\n\n\n2006\n7.652\n2.180\n2.627\n\n\n2008\nNaN\n2.211\n2.720\n\n\n2009\nNaN\n2.234\n2.734\n\n\n2011\n8.174\n2.250\n2.760\n\n\n2012\n8.293\n2.244\n2.627\n\n\n2015\n8.615\n2.210\nNaN\n\n\n2019\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe can see that his has moved the Year column to become the index.\n\n\n\nVisualise your data\nPandas comes with some tools for displaying tables of data visually. We won’t cover the details of manipulating these plots here but for quickly checking the shape of the data, it’s incredibly useful. It’s a good idea to always plot your data once you’ve read it in as it will often show issues with the data more clearly than by scanning table of numbers.\nIf you have a variable containing a DataFrame (like we do with census), you can plot it as a line graph using:\n\ncensus.plot()\n\n\n\n\n\n\n\n\nFrom this we can quickly see the missing data showing as gaps in the graph, and also that there are no clearly anomalous entries.\nIf you want to dive deeper into how this graph can be improved visually, you can see a short aside which covers that, but which does use some tools that we will not cover until later chapters.\n\n\n\n\n\n\nExercise\n\n\n\nRead the file at https://milliams.com/courses/data_analysis_python/meantemp_monthly_totals.txt into Pandas (this data is originally from the Met Office and there’s a description of the format there too under “Format for monthly CET series data”). This contains some historical weather data for a location in the UK. Import that file as a Pandas DataFrame using read_csv(), making sure that you set the index column, skip the appropriate rows, separate the columns correctly and cover all the possible NaN values.\nHint: This data is a little tricky to deal with as it uses spaces to separate its columns. You can’t just use sep=\" \" as that will assume that a single space is the separator. Instead of using sep at all, you need to tell it to use whitespace (e.g. spaces, tabs, etc.) as the delimiter (search the documentation for an appropriate argument).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\nFirstly, if we read the data in without passing any extra arguments, we get:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nMean Central England Temperature (Degrees Celsius)\n\n\n\n\n1659-1973 Manley (Q.J.R.METEOROL.SOC.\n1974)\n\n\n1974 on Parker et al. (INT.J.CLIM.\n1992)\n\n\nParker and Horton (INT.J.CLIM.\n2005)\n\n\nYear Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Annual\nNaN\n\n\n1659 3.0 4.0 6.0 7.0 11.0 13.0 16.0 16.0 13.0 10.0 5.0 2.0 8.9\nNaN\n\n\n\n\n\n\n\nSo we need to dot he same as before, setting the skiprows argument:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,  # skip first 4 rows of the header\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nYear Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Annual\n\n\n\n\n0\n1659 3.0 4.0 6.0 7.0 11.0 13...\n\n\n1\n1660 0.0 4.0 6.0 9.0 11.0 14...\n\n\n2\n1661 5.0 5.0 6.0 8.0 11.0 14...\n\n\n3\n1662 5.0 6.0 6.0 8.0 11.0 15...\n\n\n4\n1663 1.0 1.0 5.0 7.0 10.0 14...\n\n\n\n\n\n\n\nIt’s not separating the columns correctly so if we look at the data and see spaces, we might think that useing sep=\" \" would work, but if we try it:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep=\" \",  # try this...\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 1\nYear\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nJan\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\n...\nUnnamed: 43\nUnnamed: 44\nUnnamed: 45\nNov\nUnnamed: 47\nUnnamed: 48\nUnnamed: 49\nDec\nUnnamed: 51\nAnnual\n\n\n\n\n0\nNaN\nNaN\n1659\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\nNaN\n...\nNaN\n2.0\nNaN\nNaN\nNaN\n8.9\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\n1660\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n...\nNaN\n5.0\nNaN\nNaN\nNaN\n9.1\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\n1661\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\n6.0\nNaN\nNaN\nNaN\n9.8\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\n1662\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\n3.0\nNaN\nNaN\nNaN\n9.5\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n1663\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n...\nNaN\n5.0\nNaN\nNaN\nNaN\n8.6\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 53 columns\n\n\n\nThat doesn’t look right. This is because sep=\" \" means “use a single space” as the separator, but in the data most columns are separated by multiple spaces. To make it use “any number of spaces” as the separator, you can instead set sep='\\s+':\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',  # whitespace-separated columns\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\n\n\n0\n1659\n3.0\n4.0\n6.0\n7.0\n11.0\n13.0\n16.0\n16.0\n13.0\n10.0\n5.0\n2.0\n8.9\n\n\n1\n1660\n0.0\n4.0\n6.0\n9.0\n11.0\n14.0\n15.0\n16.0\n13.0\n10.0\n6.0\n5.0\n9.1\n\n\n2\n1661\n5.0\n5.0\n6.0\n8.0\n11.0\n14.0\n15.0\n15.0\n13.0\n11.0\n8.0\n6.0\n9.8\n\n\n3\n1662\n5.0\n6.0\n6.0\n8.0\n11.0\n15.0\n15.0\n15.0\n13.0\n11.0\n6.0\n3.0\n9.5\n\n\n4\n1663\n1.0\n1.0\n5.0\n7.0\n10.0\n14.0\n15.0\n15.0\n13.0\n10.0\n7.0\n5.0\n8.6\n\n\n\n\n\n\n\nThat looks much better! Now we set the index_col:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',\n    index_col=\"Year\",  # Set the index\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1659\n3.0\n4.0\n6.0\n7.0\n11.0\n13.0\n16.0\n16.0\n13.0\n10.0\n5.0\n2.0\n8.9\n\n\n1660\n0.0\n4.0\n6.0\n9.0\n11.0\n14.0\n15.0\n16.0\n13.0\n10.0\n6.0\n5.0\n9.1\n\n\n1661\n5.0\n5.0\n6.0\n8.0\n11.0\n14.0\n15.0\n15.0\n13.0\n11.0\n8.0\n6.0\n9.8\n\n\n1662\n5.0\n6.0\n6.0\n8.0\n11.0\n15.0\n15.0\n15.0\n13.0\n11.0\n6.0\n3.0\n9.5\n\n\n1663\n1.0\n1.0\n5.0\n7.0\n10.0\n14.0\n15.0\n15.0\n13.0\n10.0\n7.0\n5.0\n8.6\n\n\n\n\n\n\n\nAnd, as we should always do, we plot the data we’ve just read in:\n\ntemperature.plot()\n\n\n\n\n\n\n\n\nSomething is wrong with this. There’s a line on the right-hand side which seems wrong. If we look at the last few lines of the data to see what’s going on:\n\ntemperature.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n5.3\n3.1\n5.0\n9.9\n13.3\n16.1\n19.3\n16.8\n13.7\n10.5\n8.3\n6.8\n10.7\n\n\n2019\n4.0\n6.9\n7.9\n9.1\n11.2\n14.2\n17.6\n17.2\n14.3\n9.8\n6.2\n5.7\n10.4\n\n\n2020\n6.4\n6.4\n6.8\n10.5\n12.6\n15.3\n15.8\n17.7\n14.0\n10.4\n8.5\n4.9\n10.8\n\n\n2021\n3.2\n5.3\n7.3\n6.5\n10.3\n15.5\n17.8\n16.0\n16.0\n12.0\n7.2\n6.3\n10.3\n\n\n2022\n4.7\n6.9\n8.0\n9.2\n13.1\n14.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n\n\n\n\n\n\n\nWe can see there are some -99.9 in the data, repsenting missing data. We should fix this with na_values:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',\n    index_col=\"Year\",\n    na_values=[\"-99.9\"]\n)\ntemperature.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n5.3\n3.1\n5.0\n9.9\n13.3\n16.1\n19.3\n16.8\n13.7\n10.5\n8.3\n6.8\n10.7\n\n\n2019\n4.0\n6.9\n7.9\n9.1\n11.2\n14.2\n17.6\n17.2\n14.3\n9.8\n6.2\n5.7\n10.4\n\n\n2020\n6.4\n6.4\n6.8\n10.5\n12.6\n15.3\n15.8\n17.7\n14.0\n10.4\n8.5\n4.9\n10.8\n\n\n2021\n3.2\n5.3\n7.3\n6.5\n10.3\n15.5\n17.8\n16.0\n16.0\n12.0\n7.2\n6.3\n10.3\n\n\n2022\n4.7\n6.9\n8.0\n9.2\n13.1\n14.9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ntemperature.plot()\n\n\n\n\n\n\n\n\n\n\n\nThat covers the basics of reading in data with pandas. Next we will be looking at how we can query detailed information from our data.",
    "crumbs": [
      "Reading from file"
    ]
  },
  {
    "objectID": "pages/98 Summary.html",
    "href": "pages/98 Summary.html",
    "title": "Summary",
    "section": "",
    "text": "This course has given you a brief introduction to the tools that can help you read in, analyse and plot your data.\nOnce you’re happy with the concepts in this course, you may want to look at the Applied Data Analysis in Python course which starts to show how to apply more advanced techniques such as machine learning to data.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "pages/05 Visualisation.html",
    "href": "pages/05 Visualisation.html",
    "title": "Visualising data",
    "section": "",
    "text": "The de facto standard plotting and visualisation library for Python is matplotlib. It is incredibly versatile and it is worth getting to know. However, its API can be a little complex and can require a fair bit of code to get plot looking how you want.\nThere are libraries which build on top of matplotlib to provide a nice interface, while still allowing you to peek beneath the surface to access the full power of matplotlib. We’ve seen that pandas can do some plotting directly from DataFrames and we’ll look at another in this chapter, the popular seaborn library.\n\nseaborn\nThe built-in pandas plotting we saw earlier is really useful because it is always available at your fingertips and it smooths over some of the tricky parts of matplotlib, but it will only take you so far.\nseaborn is a third-party library which provides an easy-to-use interface for plotting tabular data which integrates with pandas really well. By convention it is imported as sns:\n\nimport seaborn as sns\n\nseaborn comes with its own style which affects the colours, grid, fonts etc. You don’t have to use the seaborn theme but it generally looks nicer than the matplotlib defaults:\n\nsns.set_theme()\n\nNote, in versions before seaborn 0.11 this function was called sns.set(). If you are running an older version you will need to use that form.\nOnce you’ve done this, you’re all ready to get started. So let’s grab some data and start thinking about what we might want to plot. We’ll use the “tips” data set from before as it’s got some interesting features in the data for us to explore:\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\nBefore diving in, it’s worth thinking about what kinds of plots we can do and what kind of information we want to show. When communicating your results to an audience you should consider the story you are trying to tell. Not story in the “fiction” sense but story in the “narrative” sense. Visualisations should be used to discover and back up conclusions and so the choice of what you show, as well as the choice of what you don’t is important.\nThere are three major kinds of visualisation that you may want to use:\n\n\nPlotting relationships between variables in the data set\n\n\nThis is showing how a change in one variable affects another variable. For example it could be scatter plot of how age and weight are related or a line plot of the change in temperature over time.\n\n\nPlotting the distribution of variables\n\n\nThis is usually about how many of each value you have and how they are spread out. For example it could be a histogram of the spread of marks for a class of students or the ages of people in a country.\n\n\nSeeing how the data varies by category\n\n\nCategories in your data can be used in conjunction with both the methods described above but sometimes splitting your data by category and looking at each separately is a good choice. For example it could be a bar chart of how much each country spent on foreign aid.\n\n\n\nPlotting relationships\nWe’ll start with looking at how we can visualise how data are related to each other. seaborn offers a generic interface for doing this called relplot(). You can find more information in the seaborn documentation for the relplot() function and the more general page on visualising statistical relationships.\nYou use relplot() by passing it the DataFrame you want to explore and the two dimensions of the table that you want to compare. This only makes sense for dimensions that are numerical (e.g. height, weight, price, etc.) and not for “categorical” data like “day of the week” or “species”. The numerical variables in our data are total_bill, tip and size.\nThe data argument should be the entire DataFrame and the x and y arguments should be the names of the columns to plot. So, to plot the values of tips against the total bill, we do:\n\nsns.relplot(\n    data=tips,\n    x=\"total_bill\",\n    y=\"tip\",\n)\n\n\n\n\n\n\n\n\nBy default it creates a scatter plot and uses the column names as axis labels. To set the axis labels to something more useful, you can call set() on the object returned by relplot():\n\nsns.relplot(\n    data=tips,\n    x=\"total_bill\",\n    y=\"tip\",\n).set(\n    xlabel=\"Total bill (£)\",\n    ylabel=\"Tip (£)\",\n)\n\n\n\n\n\n\n\n\nIn the example above we chose two variables from our table of data and used them as a visual dimension in our plot. We wanted the total bill to be represented as the distance from left-to-right and the tip amount to be represented as the distance from bottom-to-top.\nseaborn provides a bunch of different dimensions onto which you can map your data:\n\nx=: see above\ny=: see above\nhue=: assign a different colour depending on the value. If the variable is numerical then it will vary the colour smoothly, if it is categorical then it will assign discrete colours to each category.\nsize=: vary the size of the marker depending on the value\nstyle=: use a different marker type (e.g. ▲, ◆, ●, ◼) for the variable. Only makes sense for discrete variables.\nrow= and col=: for discrete variables, draw a separate subplot along a row or column for each category\n\nIf you have multiple dimensions to visualise, you can therefore plot up to 7 dimensional data. Here’s an example using 5 dimensions for different variables:\n\nsns.relplot(\n    data=tips,\n    x=\"total_bill\",\n    y=\"tip\",\n    hue=\"day\",\n    size=\"size\",\n    style=\"time\",\n).set(\n    xlabel=\"Total bill (£)\",\n    ylabel=\"Tip (£)\",\n)\n\n\n\n\n\n\n\n\nBut this very quickly gets very hard to interpret and understand. It is instead recommended to keep the number of variables plotted at once small and instead use the plotting dimensions redundantly, i.e. using both the marker style and the hue for the same data variable. This can make the plot more accessible when printed in black-and-white or when viewed by someone with colour-blindness:\n\nsns.relplot(\n    data=tips,\n    x=\"total_bill\",\n    y=\"tip\",\n    hue=\"day\",\n    style=\"day\",\n).set(\n    xlabel=\"Total bill (£)\",\n    ylabel=\"Tip (£)\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing the bill_per_person and percent_tip columns from the previous chapter, plot the relationship between the percentage tip, and the bill per-person.\nSet the colour of the data points to be based on the day of the week.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\ntips[\"percent_tip\"] = (tips[\"tip\"] / tips[\"total_bill\"])*100\n\n\nimport seaborn as sns\n\nsns.relplot(\n    data=tips,\n    x=\"bill_per_person\",\n    y=\"percent_tip\",\n    hue=\"day\",\n    style=\"day\",\n).set(\n    xlabel=\"Bill per person (£)\",\n    ylabel=\"Tip percent\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting distributions\nRather than looking at how one variable relates to another, sometimes you want to see how the data points in one particular dimension are arrayed. As with the x and y axes for relplot, this only makes sense for variables that are numerical.\nFor example, we might want to get an overview of what kind of values we have for the total bill. We could summarise this down to a few single variables like tips[\"total_bill\"].mean() and tips[\"total_bill\"].std() but this loses a lot of information.\nseaborn provides a function for investigating the distribution of a variable called displot which works in a similar way to relplot. You pass the data frame you want to visualise and then start specifying the dimensions that you want to show.\nNote: displot was added in version 0.11 of seaborn. If you are on an earlier version then you will not have it available. In this case you should upgrade your version of seaborn to at least 0.11. If you cannot upgrade then you can make histograms using the distplot function (note the extra t), though it lacks some features.\nFor example, to show a histogram of the total bill, you do the following:\n\nsns.displot(\n    data=tips,\n    x=\"total_bill\",\n)\n\n\n\n\n\n\n\n\nAs with relplot we can ask seaborn to show additional dimensions of our data with the following arguments (which have the same meaning as in relplot): x, y, hue, row and col.\nSo, to compare the distribution of total bills between lunch time and dinner time, arraying them by column, you can do:\n\nsns.displot(\n    data=tips,\n    x=\"total_bill\",\n    col=\"time\",  # Added this line\n)\n\n\n\n\n\n\n\n\nSince displot shows the count of the values in each bin by default, the first thing that you would conclude is that there are more dinner data than lunch. If this is the story you are exploring then it’s all good. However, we want to show how the distribution varies from lunch to dinner. To have it ignore the absolute values of the counts and to instead normalise the counts within each category, you can set common_norm=False which allows each subset to normalise individually:\n\nsns.displot(\n    data=tips,\n    x=\"total_bill\",\n    col=\"time\",\n    stat=\"density\",  # Added this line\n    common_norm=False,  # Added this line\n)\n\n\n\n\n\n\n\n\nWe can now see that the lunch bills have a higher single peak which suggests that it has a tighter distribution. However, plotting them side-by-side makes it difficult to compare the point at which the peak happens in each data set. Do they align or are they shifted?\nTo solve this, we can plot the two overlaid by using the hue semantic instead of the col:\n\nsns.displot(\n    data=tips,\n    x=\"total_bill\",\n    hue=\"time\",  # Changed this line\n    stat=\"density\",\n    common_norm=False,\n)\n\n\n\n\n\n\n\n\nWe can now see that the lunch orders peak at a similar value to dinner but are shifted slightly to the left. They also have a tighter distribution.\nTo simplify the view down further, you can smooth the bins by applying a “kernel density estimation” which allows you to turn it into a line graph:\n\nsns.displot(\n    data=tips,\n    x=\"total_bill\",\n    hue=\"time\",\n    kind=\"kde\",  # Changed this line\n    common_norm=False,\n)\n\n\n\n\n\n\n\n\nNow we can directly see that the lunch peak is tighter and peaks slightly lower.\n\n\n\n\n\n\nExercise 2\n\n\n\nInvestigate how the time of day affects how much each person spends on average.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\n\n\nimport seaborn as sns\n\nsns.displot(data=tips, x=\"bill_per_person\", hue=\"time\", kind=\"kde\", common_norm=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting categorical data\nWhen you have categorical variables in your data, you usually want to compare between the categories. seaborn’s catplot and the associated tutorial page are the solution here.\nThe categorical variables in our data set are day and time. If we wanted to see how the total bill depended on the day of the week, we could do the following:\n\nsns.catplot(\n    data=tips,\n    x=\"day\",\n    y=\"total_bill\",\n)\n\n\n\n\n\n\n\n\nWhile the day of week is a categorical variable, it also has an understood common ordering to it where we would expect the days to presented in that order. We can specify the ordering with the order parameter:\n\nsns.catplot(\n    data=tips,\n    x=\"day\",\n    y=\"total_bill\",\n    order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"],\n)\n\n\n\n\n\n\n\n\nThis is showing that it seems like Thursday and Friday have a slightly lower average that the other days. The default “strip plot” is good because it doesn’t summarise the data too much, but when looking for averages something like a box plot might be better. Pass the argument kind=\"box\" to do this:\n\nsns.catplot(\n    data=tips,\n    x=\"day\",\n    y=\"total_bill\",\n    order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"],\n    kind=\"box\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThere seems to be a trend towards larger total bills at the weekend. Investigate whether this is due to a larger average spend per person or due to a larger average group size.\nSee if using kind=\"violin\" instead of \"box\" presents your data any better.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\n\n\nimport seaborn as sns\n\n\nsns.catplot(data=tips, x=\"day\", y=\"bill_per_person\", order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"], kind=\"box\")\n\n\n\n\n\n\n\n\n\nsns.catplot(data=tips, x=\"day\", y=\"size\", order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"], kind=\"violin\")",
    "crumbs": [
      "Visualising data"
    ]
  },
  {
    "objectID": "pages/06 Final exercise.html",
    "href": "pages/06 Final exercise.html",
    "title": "Final exercise",
    "section": "",
    "text": "For the final exercise you should try to combine together all the skills you’ve learned over this course. This is expected to take you longer than any of the exercises so far. If you get to this point in the session, feel free to get started on it now, otherwise you can treat this as practice later on.\nRead the file https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/titanic.csv. This file contains information about all the passengers and crew aboard the RMS Titanic. Its columns are:\n\nname: strings with the name of the passenger.\ngender: “male” or “female”.\nage: a float with the persons age on the day of the sinking. The age of babies (under 12 months) is given as a fraction of one year.\nclass: a string specifying the class (1st, 2nd or 3rd) for passengers or the type of service aboard for crew members.\nembarked: the persons place of of embarkment.\ncountry: the persons home country.\nticketno: the persons ticket number (NA for crew members).\nfare: the ticket price (NA for crew members, musicians and employees of the shipyard company).\nsibsp: a number specifying the number if siblings/spouses aboard.\nparch: a number specifying the number of parents/children aboard.\nsurvived: a string (“no” or “yes”) specifying whether the person has survived the sinking.\n\nWork through the following suggested questions. Feel free to go off-course and explore whatever you think is interesting in the data.\n\nSummarising\n\nFind the average age of all people on board\nUse a filter to select only the males\nFind the average age of the males on board\n\n\n\nFiltering\n\nUse a filter to select only the people in 3rd class.\nCreate a DataFrame which only contains the passengers on the ship (those in first, second or third class).\n\n\n\nPlotting\n\nPlot and compare the distribution of ages for males and females.\nHow does this differ by class?\n\n\n\nCombining\nCalculate the percentage that survived within each class.\n\n\nExplore\nExplore the data and see what you can find. For example, try exploring what the main factors predicting survival were.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere’s some help with the answers to some parts of this exercise here.\n\nimport pandas as pd\nimport seaborn as sns\ntitanic = pd.read_csv(\"./data/titanic.csv\")\ntitanic\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsibsp\nparch\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.11\n0.0\n0.0\nno\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0.0\n2.0\nno\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.05\n1.0\n1.0\nno\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.05\n1.0\n1.0\nyes\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.13\n0.0\n0.0\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2202\nWynn, Mr. Walter\nmale\n41.0\ndeck crew\nB\nEngland\nNaN\nNaN\nNaN\nNaN\nyes\n\n\n2203\nYearsley, Mr. Harry\nmale\n40.0\nvictualling crew\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nyes\n\n\n2204\nYoung, Mr. Francis James\nmale\n32.0\nengineering crew\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n2205\nZanetti, Sig. Minio\nmale\n20.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n2206\nZarracchi, Sig. L.\nmale\n26.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n\n\n2207 rows × 11 columns\n\n\n\n\nSummarising\n\nFind the average age of all people on board\n\ntitanic[\"age\"].mean()\n\n30.436734693877504\n\n\n\n\nUse a filter to select only the males\n\nall_males = titanic[titanic[\"gender\"] == \"male\"]\n\n\n\nFind the average age of the males on board\n\nall_males[\"age\"].mean()\n\n30.83231351981346\n\n\n\n\n\nFiltering\n\nSelect on the people in 3rd class\n\ntitanic[titanic[\"class\"] == \"3rd\"]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsibsp\nparch\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.1100\n0.0\n0.0\nno\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.0500\n0.0\n2.0\nno\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.0500\n1.0\n1.0\nno\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.0500\n1.0\n1.0\nyes\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.1300\n0.0\n0.0\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1313\nYūsuf, Mrs. Kātrīn\nfemale\n23.0\n3rd\nC\nLebanon\n2668.0\n22.0702\n0.0\n2.0\nyes\n\n\n1315\nZakarian, Mr. Mapriededer\nmale\n22.0\n3rd\nC\nTurkey\n2656.0\n7.0406\n0.0\n0.0\nno\n\n\n1316\nZakarian, Mr. Ortin\nmale\n27.0\n3rd\nC\nTurkey\n2670.0\n7.0406\n0.0\n0.0\nno\n\n\n1317\nZenni, Mr. Philip\nmale\n25.0\n3rd\nC\nLebanon\n2620.0\n7.0406\n0.0\n0.0\nyes\n\n\n1318\nZimmermann, Mr. Leo\nmale\n29.0\n3rd\nS\nGermany\n315082.0\n7.1706\n0.0\n0.0\nno\n\n\n\n\n709 rows × 11 columns\n\n\n\n\n\nSelect just the passengers\nThe technique shown in class was to combine together multiple selectors with |:\n\npassengers = titanic[\n    (titanic[\"class\"] == \"1st\") | \n    (titanic[\"class\"] == \"2nd\") | \n    (titanic[\"class\"] == \"3rd\")\n]\n\nHowever, it is also possible to use the isin method to select from a list of matching options:\n\npassengers = titanic[titanic[\"class\"].isin([\"1st\", \"2nd\", \"3rd\"])]\n\n\n\n\nPlotting\n\nPlot the distribution of ages for males and females\nUsing displot with age as the main variable shows the distribution. YOu can overlay the two genders using hue=\"gender\". To simplify the view, you can set kind=\"kde\". Since KDE mode smooths the data, you can also set a cutoff of 0 to avoid it showing negative ages:\n\nsns.displot(\n    data=passengers,\n    x=\"age\",\n    hue=\"gender\",\n    kind=\"kde\",\n    cut=0\n)\n\n\n\n\n\n\n\n\n\n\nHow does this differ by class?\nAll that has changed from the last plot is adding in the split by class over multiple columns:\n\nsns.displot(\n    data=passengers,\n    x=\"age\",\n    hue=\"gender\",\n    kind=\"kde\",\n    cut=0,\n    col=\"class\",\n    col_order=[\"1st\", \"2nd\", \"3rd\"]\n)\n\n\n\n\n\n\n\n\n\n\n\nCombining\nTo reduce the duplication of effort here, I create a function which, given a set of data, calculated the survived fraction within. This is then called three times, once for each class:\n\ndef survived_ratio(df):\n    yes = df[df[\"survived\"] == \"yes\"]\n    return len(yes) / len(df)\n\nratio_1st = survived_ratio(passengers[passengers[\"class\"] == \"1st\"])\nratio_2nd = survived_ratio(passengers[passengers[\"class\"] == \"2nd\"])\nratio_3rd = survived_ratio(passengers[passengers[\"class\"] == \"3rd\"])\n\nprint(ratio_1st, ratio_2nd, ratio_3rd)\n\n0.6203703703703703 0.4154929577464789 0.2552891396332863",
    "crumbs": [
      "Final exercise"
    ]
  },
  {
    "objectID": "appendix/aside_census_plot.html",
    "href": "appendix/aside_census_plot.html",
    "title": "Improving the look of a plot",
    "section": "",
    "text": "This aside will show how we can go about improving the visuals of this graph. This will use some of the topics that we will be covering in later chapters, so you might want to come back to this aside once you’ve been through the material in the visualisation chapter.\n\nimport pandas as pd\n\ncity_pop_file = \"https://milliams.com/courses/data_analysis_python/city_pop.csv\"\ncensus = pd.read_csv(\n    city_pop_file,\n    skiprows=5,\n    sep=\";\",\n    na_values=\"-1\",\n    index_col=\"year\",\n)\ncensus\n\n\n\n\n\n\n\n\nLondon\nParis\nRome\n\n\nyear\n\n\n\n\n\n\n\n2001\n7.322\nNaN\n2.547\n\n\n2006\n7.652\n2.180\n2.627\n\n\n2008\nNaN\n2.211\n2.720\n\n\n2009\nNaN\n2.234\n2.734\n\n\n2011\n8.174\n2.250\n2.760\n\n\n2012\n8.293\n2.244\n2.627\n\n\n2015\n8.615\n2.210\nNaN\n\n\n2019\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe simplest thing you can do is plot the graph with no additional options:\n\ncensus.plot()\n\n\n\n\n\n\n\n\nThe label on the x-axis is taken directly from the column name that we made into the index, \"year\". Let’s make it have a capital letter at the start by passing the xlabel argument to plot:\n\ncensus.plot(\n    xlabel=\"Year\",\n)\n\n\n\n\n\n\n\n\nAnd then also set a y-axis label in a similar way:\n\ncensus.plot(\n    xlabel=\"Year\",\n    ylabel=\"Population (millions)\",\n)\n\n\n\n\n\n\n\n\nThe y-axis currently starts around 2 which makes the difference between London and the other cities look greater than it actually is. It’s usually a good idea to set your y-axis to start at zero. We can pass a tuple (0, None) to the ylim argument which tells the y-axis to start at 0 and the None tells it to use the default scale for the upper bound:\n\ncensus.plot(\n    xlabel=\"Year\",\n    ylabel=\"Population (millions)\",\n    ylim=(0, None),\n)\n\n\n\n\n\n\n\n\nThis is now a perfectly functional graph. All we might want to do now is to play with the aesthetics a little. Using seaborn we can use their theme which can use nicer fonts and colours:\n\nimport seaborn as sns\n\nsns.set_theme()\n\ncensus.plot(\n    xlabel=\"Year\",\n    ylabel=\"Population (millions)\",\n    ylim=(0, None),\n)\n\n\n\n\n\n\n\n\nIf we want a white background again, we can specify the seaborn style with sns.set_style:\n\nsns.set_style(\"white\")\n\ncensus.plot(\n    xlabel=\"Year\",\n    ylabel=\"Population (millions)\",\n    ylim=(0, None),\n)\n\n\n\n\n\n\n\n\nOr, if we want, we can use seaborn directly as the plotting tool using seaborn’s sns.relplot:\n\nsns.relplot(data=census, kind=\"line\").set(\n    xlabel=\"Year\",\n    ylabel=\"Population (millions)\",\n    ylim=(0, None),\n)\n\n\n\n\n\n\n\n\nReturn to course"
  },
  {
    "objectID": "answers/answer_analysis_per_person.html",
    "href": "answers/answer_analysis_per_person.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nbill_per_person = tips[\"total_bill\"] / tips[\"size\"]\nbill_per_person\n\n0       8.495000\n1       3.446667\n2       7.003333\n3      11.840000\n4       6.147500\n         ...    \n239     9.676667\n240    13.590000\n241    11.335000\n242     8.910000\n243     9.390000\nLength: 244, dtype: float64\n\n\n\nbill_per_person.mean()\n\n7.888229508196722\n\n\n\nbill_per_person.std()\n\n2.9143496626221"
  },
  {
    "objectID": "answers/answer_filtering_tip_day.html",
    "href": "answers/answer_filtering_tip_day.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nthurs = tips[tips[\"day\"] == \"Thur\"]\nthurs\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n77\n27.20\n2.80\nThur\nLunch\n4\n\n\n78\n22.76\n2.10\nThur\nLunch\n2\n\n\n79\n17.29\n1.90\nThur\nLunch\n2\n\n\n80\n19.44\n2.10\nThur\nLunch\n2\n\n\n81\n16.66\n2.38\nThur\nLunch\n2\n\n\n...\n...\n...\n...\n...\n...\n\n\n202\n13.00\n1.40\nThur\nLunch\n2\n\n\n203\n16.40\n1.75\nThur\nLunch\n2\n\n\n204\n20.53\n2.80\nThur\nLunch\n4\n\n\n205\n16.47\n2.26\nThur\nLunch\n3\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n\n\n\n\n62 rows × 5 columns\n\n\n\n\nthurs[\"tip\"].mean()\n\n1.9398387096774192\n\n\n\ntips[tips[\"day\"] == \"Sat\"][\"tip\"].mean()\n\n2.095402298850575"
  },
  {
    "objectID": "answers/answer_read_weather.html",
    "href": "answers/answer_read_weather.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\nFirstly, if we read the data in without passing any extra arguments, we get:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nMean Central England Temperature (Degrees Celsius)\n\n\n\n\n1659-1973 Manley (Q.J.R.METEOROL.SOC.\n1974)\n\n\n1974 on Parker et al. (INT.J.CLIM.\n1992)\n\n\nParker and Horton (INT.J.CLIM.\n2005)\n\n\nYear Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Annual\nNaN\n\n\n1659 3.0 4.0 6.0 7.0 11.0 13.0 16.0 16.0 13.0 10.0 5.0 2.0 8.9\nNaN\n\n\n\n\n\n\n\nSo we need to dot he same as before, setting the skiprows argument:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,  # skip first 4 rows of the header\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nYear Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Annual\n\n\n\n\n0\n1659 3.0 4.0 6.0 7.0 11.0 13...\n\n\n1\n1660 0.0 4.0 6.0 9.0 11.0 14...\n\n\n2\n1661 5.0 5.0 6.0 8.0 11.0 14...\n\n\n3\n1662 5.0 6.0 6.0 8.0 11.0 15...\n\n\n4\n1663 1.0 1.0 5.0 7.0 10.0 14...\n\n\n\n\n\n\n\nIt’s not separating the columns correctly so if we look at the data and see spaces, we might think that useing sep=\" \" would work, but if we try it:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep=\" \",  # try this...\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 1\nYear\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nJan\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\n...\nUnnamed: 43\nUnnamed: 44\nUnnamed: 45\nNov\nUnnamed: 47\nUnnamed: 48\nUnnamed: 49\nDec\nUnnamed: 51\nAnnual\n\n\n\n\n0\nNaN\nNaN\n1659\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\nNaN\n...\nNaN\n2.0\nNaN\nNaN\nNaN\n8.9\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\n1660\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n...\nNaN\n5.0\nNaN\nNaN\nNaN\n9.1\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\n1661\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\n6.0\nNaN\nNaN\nNaN\n9.8\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\n1662\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\n3.0\nNaN\nNaN\nNaN\n9.5\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n1663\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n...\nNaN\n5.0\nNaN\nNaN\nNaN\n8.6\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 53 columns\n\n\n\nThat doesn’t look right. This is because sep=\" \" means “use a single space” as the separator, but in the data most columns are separated by multiple spaces. To make it use “any number of spaces” as the separator, you can instead set sep='\\s+':\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',  # whitespace-separated columns\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\n\n\n0\n1659\n3.0\n4.0\n6.0\n7.0\n11.0\n13.0\n16.0\n16.0\n13.0\n10.0\n5.0\n2.0\n8.9\n\n\n1\n1660\n0.0\n4.0\n6.0\n9.0\n11.0\n14.0\n15.0\n16.0\n13.0\n10.0\n6.0\n5.0\n9.1\n\n\n2\n1661\n5.0\n5.0\n6.0\n8.0\n11.0\n14.0\n15.0\n15.0\n13.0\n11.0\n8.0\n6.0\n9.8\n\n\n3\n1662\n5.0\n6.0\n6.0\n8.0\n11.0\n15.0\n15.0\n15.0\n13.0\n11.0\n6.0\n3.0\n9.5\n\n\n4\n1663\n1.0\n1.0\n5.0\n7.0\n10.0\n14.0\n15.0\n15.0\n13.0\n10.0\n7.0\n5.0\n8.6\n\n\n\n\n\n\n\nThat looks much better! Now we set the index_col:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',\n    index_col=\"Year\",  # Set the index\n)\ntemperature.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1659\n3.0\n4.0\n6.0\n7.0\n11.0\n13.0\n16.0\n16.0\n13.0\n10.0\n5.0\n2.0\n8.9\n\n\n1660\n0.0\n4.0\n6.0\n9.0\n11.0\n14.0\n15.0\n16.0\n13.0\n10.0\n6.0\n5.0\n9.1\n\n\n1661\n5.0\n5.0\n6.0\n8.0\n11.0\n14.0\n15.0\n15.0\n13.0\n11.0\n8.0\n6.0\n9.8\n\n\n1662\n5.0\n6.0\n6.0\n8.0\n11.0\n15.0\n15.0\n15.0\n13.0\n11.0\n6.0\n3.0\n9.5\n\n\n1663\n1.0\n1.0\n5.0\n7.0\n10.0\n14.0\n15.0\n15.0\n13.0\n10.0\n7.0\n5.0\n8.6\n\n\n\n\n\n\n\nAnd, as we should always do, we plot the data we’ve just read in:\n\ntemperature.plot()\n\n\n\n\n\n\n\n\nSomething is wrong with this. There’s a line on the right-hand side which seems wrong. If we look at the last few lines of the data to see what’s going on:\n\ntemperature.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n5.3\n3.1\n5.0\n9.9\n13.3\n16.1\n19.3\n16.8\n13.7\n10.5\n8.3\n6.8\n10.7\n\n\n2019\n4.0\n6.9\n7.9\n9.1\n11.2\n14.2\n17.6\n17.2\n14.3\n9.8\n6.2\n5.7\n10.4\n\n\n2020\n6.4\n6.4\n6.8\n10.5\n12.6\n15.3\n15.8\n17.7\n14.0\n10.4\n8.5\n4.9\n10.8\n\n\n2021\n3.2\n5.3\n7.3\n6.5\n10.3\n15.5\n17.8\n16.0\n16.0\n12.0\n7.2\n6.3\n10.3\n\n\n2022\n4.7\n6.9\n8.0\n9.2\n13.1\n14.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n-99.9\n\n\n\n\n\n\n\nWe can see there are some -99.9 in the data, repsenting missing data. We should fix this with na_values:\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/meantemp_monthly_totals.txt\",\n    skiprows=4,\n    sep='\\s+',\n    index_col=\"Year\",\n    na_values=[\"-99.9\"]\n)\ntemperature.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n5.3\n3.1\n5.0\n9.9\n13.3\n16.1\n19.3\n16.8\n13.7\n10.5\n8.3\n6.8\n10.7\n\n\n2019\n4.0\n6.9\n7.9\n9.1\n11.2\n14.2\n17.6\n17.2\n14.3\n9.8\n6.2\n5.7\n10.4\n\n\n2020\n6.4\n6.4\n6.8\n10.5\n12.6\n15.3\n15.8\n17.7\n14.0\n10.4\n8.5\n4.9\n10.8\n\n\n2021\n3.2\n5.3\n7.3\n6.5\n10.3\n15.5\n17.8\n16.0\n16.0\n12.0\n7.2\n6.3\n10.3\n\n\n2022\n4.7\n6.9\n8.0\n9.2\n13.1\n14.9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ntemperature.plot()"
  },
  {
    "objectID": "answers/answer_visualisation_catplot.html",
    "href": "answers/answer_visualisation_catplot.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\n\n\nimport seaborn as sns\n\n\nsns.catplot(data=tips, x=\"day\", y=\"bill_per_person\", order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"], kind=\"box\")\n\n\n\n\n\n\n\n\n\nsns.catplot(data=tips, x=\"day\", y=\"size\", order=[\"Thur\", \"Fri\", \"Sat\", \"Sun\"], kind=\"violin\")"
  },
  {
    "objectID": "answers/answer_visualisation_displot.html",
    "href": "answers/answer_visualisation_displot.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\n\n\nimport seaborn as sns\n\nsns.displot(data=tips, x=\"bill_per_person\", hue=\"time\", kind=\"kde\", common_norm=False)"
  },
  {
    "objectID": "answers/answer_final_exercise.html",
    "href": "answers/answer_final_exercise.html",
    "title": "Summarising",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\ntitanic = pd.read_csv(\"./data/titanic.csv\")\ntitanic\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsibsp\nparch\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.11\n0.0\n0.0\nno\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0.0\n2.0\nno\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.05\n1.0\n1.0\nno\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.05\n1.0\n1.0\nyes\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.13\n0.0\n0.0\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2202\nWynn, Mr. Walter\nmale\n41.0\ndeck crew\nB\nEngland\nNaN\nNaN\nNaN\nNaN\nyes\n\n\n2203\nYearsley, Mr. Harry\nmale\n40.0\nvictualling crew\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nyes\n\n\n2204\nYoung, Mr. Francis James\nmale\n32.0\nengineering crew\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n2205\nZanetti, Sig. Minio\nmale\n20.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n2206\nZarracchi, Sig. L.\nmale\n26.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\nNaN\nNaN\nno\n\n\n\n\n2207 rows × 11 columns\n\n\n\n\nFind the average age of all people on board\n\ntitanic[\"age\"].mean()\n\n30.436734693877504\n\n\n\n\nUse a filter to select only the males\n\nall_males = titanic[titanic[\"gender\"] == \"male\"]\n\n\n\nFind the average age of the males on board\n\nall_males[\"age\"].mean()\n\n30.83231351981346\n\n\n\n\nFiltering\n\nSelect on the people in 3rd class\n\ntitanic[titanic[\"class\"] == \"3rd\"]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsibsp\nparch\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.1100\n0.0\n0.0\nno\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.0500\n0.0\n2.0\nno\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.0500\n1.0\n1.0\nno\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.0500\n1.0\n1.0\nyes\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.1300\n0.0\n0.0\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1313\nYūsuf, Mrs. Kātrīn\nfemale\n23.0\n3rd\nC\nLebanon\n2668.0\n22.0702\n0.0\n2.0\nyes\n\n\n1315\nZakarian, Mr. Mapriededer\nmale\n22.0\n3rd\nC\nTurkey\n2656.0\n7.0406\n0.0\n0.0\nno\n\n\n1316\nZakarian, Mr. Ortin\nmale\n27.0\n3rd\nC\nTurkey\n2670.0\n7.0406\n0.0\n0.0\nno\n\n\n1317\nZenni, Mr. Philip\nmale\n25.0\n3rd\nC\nLebanon\n2620.0\n7.0406\n0.0\n0.0\nyes\n\n\n1318\nZimmermann, Mr. Leo\nmale\n29.0\n3rd\nS\nGermany\n315082.0\n7.1706\n0.0\n0.0\nno\n\n\n\n\n709 rows × 11 columns\n\n\n\n\n\nSelect just the passengers\nThe technique shown in class was to combine together multiple selectors with |:\n\npassengers = titanic[\n    (titanic[\"class\"] == \"1st\") | \n    (titanic[\"class\"] == \"2nd\") | \n    (titanic[\"class\"] == \"3rd\")\n]\n\nHowever, it is also possible to use the isin method to select from a list of matching options:\n\npassengers = titanic[titanic[\"class\"].isin([\"1st\", \"2nd\", \"3rd\"])]\n\n\n\n\nPlotting\n\nPlot the distribution of ages for males and females\nUsing displot with age as the main variable shows the distribution. YOu can overlay the two genders using hue=\"gender\". To simplify the view, you can set kind=\"kde\". Since KDE mode smooths the data, you can also set a cutoff of 0 to avoid it showing negative ages:\n\nsns.displot(\n    data=passengers,\n    x=\"age\",\n    hue=\"gender\",\n    kind=\"kde\",\n    cut=0\n)\n\n\n\n\n\n\n\n\n\n\nHow does this differ by class?\nAll that has changed from the last plot is adding in the split by class over multiple columns:\n\nsns.displot(\n    data=passengers,\n    x=\"age\",\n    hue=\"gender\",\n    kind=\"kde\",\n    cut=0,\n    col=\"class\",\n    col_order=[\"1st\", \"2nd\", \"3rd\"]\n)\n\n\n\n\n\n\n\n\n\n\n\nCombining\nTo reduce the duplication of effort here, I create a function which, given a set of data, calculated the survived fraction within. This is then called three times, once for each class:\n\ndef survived_ratio(df):\n    yes = df[df[\"survived\"] == \"yes\"]\n    return len(yes) / len(df)\n\nratio_1st = survived_ratio(passengers[passengers[\"class\"] == \"1st\"])\nratio_2nd = survived_ratio(passengers[passengers[\"class\"] == \"2nd\"])\nratio_3rd = survived_ratio(passengers[passengers[\"class\"] == \"3rd\"])\n\nprint(ratio_1st, ratio_2nd, ratio_3rd)\n\n0.6203703703703703 0.4154929577464789 0.2552891396332863"
  },
  {
    "objectID": "answers/answer_filtering_combined.html",
    "href": "answers/answer_filtering_combined.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[(tips[\"size\"] &gt;= 4) & (tips[\"time\"] == \"Lunch\")]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n77\n27.20\n2.80\nThur\nLunch\n4\n\n\n85\n34.83\n3.62\nThur\nLunch\n4\n\n\n119\n24.08\n2.04\nThur\nLunch\n4\n\n\n125\n29.80\n2.94\nThur\nLunch\n6\n\n\n141\n34.30\n4.69\nThur\nLunch\n6\n\n\n142\n41.19\n3.50\nThur\nLunch\n5\n\n\n143\n27.05\n3.50\nThur\nLunch\n6\n\n\n197\n43.11\n3.50\nThur\nLunch\n4\n\n\n204\n20.53\n2.80\nThur\nLunch\n4"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This course is aimed at the Python developer who wants to learn how to do useful data analysis tasks. It will focus primarily on the Python package pandas to query, combine and visualise your data as well as covering seaborn to visualise them.\nData analysis is a huge topic and we couldn’t possibly cover it all in one short course so the purpose of this workshop is to give you an introduction to some of the most useful tools and to demonstrate some of the most common problems that surface.\nIn previous courses, you’ve used the python command line program to execute scripts. This course will use a tool called Jupyter Notebooks to run your Python code. It works with the same Python code as we’ve used before but it allows interactive execution and allows you to intersperse your code with blocks of text to explain what you’re doing and embed output such as graphs directly into the page.\nTo get started, open a new Notebook as shown by your instructor.\nThroughout this course you will likely want to start a new notebook for each section of the course so name them appropriately to make it easier to find them later.\n\nGetting started\nOnce the notebook is launched, you will see a wide grey box with a blue [ ]: to the left. The grey box is an input cell where you type any Python code you want to run:\n\n# Python code can be written in 'Code' cells\nprint(\"Output appears below when the cell is run\")\nprint(\"To run a cell, press Ctrl-Enter or Shift-Enter with the cursor inside\")\nprint(\"or use the run button (▶) in the toolbar at the top\")\n\nOutput appears below when the cell is run\nTo run a cell, press Ctrl-Enter or Shift-Enter with the cursor inside\nor use the run button (▶) in the toolbar at the top\n\n\nIn your notebook, type the following in the first cell and then run it with Shift-Enter, you should see the same output:\n\na = 5\nb = 7\na + b\n\n12\n\n\nThe cells in a notebook are linked together so a variable defined in one is available in all the cells from that point on so in the second cell you can use the variables a and b:\n\na - b\n\n-2\n\n\nSome Python libraries have special integration with Jupyter notebooks and so can display their output directly into the page. For example pandas will format tables of data nicely and matplotlib will embed graphs directly:\n\nimport pandas as pd\ntemp = pd.DataFrame(\n    [3.1, 2.4, 4.8, 4.1, 3.4, 4.2],\n    columns=[\"temp (°C)\"],\n    index=pd.RangeIndex(2000, 2006, name=\"year\")\n)\ntemp\n\n\n\n\n\n\n\n\ntemp (°C)\n\n\nyear\n\n\n\n\n\n2000\n3.1\n\n\n2001\n2.4\n\n\n2002\n4.8\n\n\n2003\n4.1\n\n\n2004\n3.4\n\n\n2005\n4.2\n\n\n\n\n\n\n\n\ntemp.plot()\n\n\n\n\n\n\n\n\n\n\nMarkdown\nIf you want to write some text as documentation (like these words here) then you should label the cell as being a Markdown cell. Do that by selecting the cell and going to the dropdown at the top of the page labelled Code and changing it to Markdown.\nIt is becomming common for people to use Jupyter notebooks as a sort of lab notebook where they document their processes, interspersed with code. This style of working where you give prose and code equal weight is sometimes called literate programming.\n\n\n\n\n\n\nExercise\n\n\n\nTake the following code and break it down, chunk by chunk, interspersing it with documentation explaining what each part does using Markdown blocks:\n\nprices = {\n    \"apple\": 0.40,\n    \"banana\": 0.50,\n}\n\nmy_basket = {\n    \"apple\": 1,\n    \"banana\": 6,\n}\n\ntotal_grocery_bill = 0\nfor fruit, count in my_basket.items():\n    total_grocery_bill += prices[fruit] * count\n\nprint(f\"I owe the grocer £{total_grocery_bill:.2f}\")\n\nYou don’t need to put only one line of code per cell, it makes sense sometimes to group some lines together.\nThroughout this course, use the Jupyter Notebook to solve the problems. Follow along with the examples, typing them into your own notebooks and see how they work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "utils/generate_weather_data.html",
    "href": "utils/generate_weather_data.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data\n\nfrom urllib.parse import urlparse\nfrom pathlib import PurePath, Path\nfrom subprocess import run\n\nimport pandas as pd\n\ndef historic_station_data(site):\n    url = f\"https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/{site}data.txt\"\n    fn = Path(PurePath(urlparse(url).path).name)\n    fn.unlink(missing_ok=True)\n    run([\"wget\", url])\n\n    df = pd.read_csv(\n        fn.name,\n        skiprows=7,\n        delim_whitespace=True,\n        na_values=[\"---\"],\n        names=[\"yyyy\", \"mm\", \"tmax\", \"tmin\", \"af\", \"rain\", \"sun\", \"provisional\"],\n        parse_dates=[[\"yyyy\", \"mm\"]],\n        index_col=\"yyyy_mm\",\n    )\n\n    fn.unlink(missing_ok=True)\n\n    # There are some * and # in the file to denote things. Let's rip them out\n    for col in [\"tmax\", \"tmin\", \"af\", \"rain\", \"sun\"]:\n        if hasattr(df[col], \"str\"):\n            df[col] = pd.to_numeric(df[col].str.replace(\"[\\*#]\", \"\", regex=True))\n\n    return df\n\nrain = pd.concat(\n    [\n        historic_station_data(\"cardiff\")[\"rain\"].rename(\"Cardiff\"),\n        historic_station_data(\"stornoway\")[\"rain\"].rename(\"Stornoway\"),\n        historic_station_data(\"oxford\")[\"rain\"].rename(\"Oxford\"),\n        historic_station_data(\"armagh\")[\"rain\"].rename(\"Armagh\"),\n    ],\n    axis=1,\n)\nrain.loc[:\"2020\"].resample(\"Y\").mean().to_csv(\"rain.csv\", date_format=\"%Y\", index_label=False, float_format='%.1f')\n\n--2024-10-01 12:45:22--  https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/cardiffdata.txt\nResolving www.metoffice.gov.uk (www.metoffice.gov.uk)... 2.23.142.166\nConnecting to www.metoffice.gov.uk (www.metoffice.gov.uk)|2.23.142.166|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 30004 (29K) [text/plain]\nSaving to: ‘cardiffdata.txt’\n\n     0K .......... .......... .........                       100% 5.49M=0.005s\n\n2024-10-01 12:45:23 (5.49 MB/s) - ‘cardiffdata.txt’ saved [30004/30004]\n\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df = pd.read_csv(\n--2024-10-01 12:45:23--  https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/stornowaydata.txt\nResolving www.metoffice.gov.uk (www.metoffice.gov.uk)... 2.23.142.166\nConnecting to www.metoffice.gov.uk (www.metoffice.gov.uk)|2.23.142.166|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/plain]\nSaving to: ‘stornowaydata.txt’\n\n     0K .......... .......... .......... .......... .......... 6.82M\n    50K .......... .......... .......... .......... ..         11.5M=0.01s\n\n2024-10-01 12:45:23 (8.41 MB/s) - ‘stornowaydata.txt’ saved [95133]\n\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df = pd.read_csv(\n--2024-10-01 12:45:23--  https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/oxforddata.txt\nResolving www.metoffice.gov.uk (www.metoffice.gov.uk)... 2.23.142.166\nConnecting to www.metoffice.gov.uk (www.metoffice.gov.uk)|2.23.142.166|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/plain]\nSaving to: ‘oxforddata.txt’\n\n     0K .......... .......... .......... .......... .......... 5.92M\n    50K .......... .......... .......... .......... .......... 10.6M\n   100K .....                                                   221M=0.01s\n\n2024-10-01 12:45:23 (7.97 MB/s) - ‘oxforddata.txt’ saved [107803]\n\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df = pd.read_csv(\n--2024-10-01 12:45:23--  https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/armaghdata.txt\nResolving www.metoffice.gov.uk (www.metoffice.gov.uk)... 2.23.142.166\nConnecting to www.metoffice.gov.uk (www.metoffice.gov.uk)|2.23.142.166|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/plain]\nSaving to: ‘armaghdata.txt’\n\n     0K .......... .......... .......... .......... .......... 6.06M\n    50K .......... .......... .......... .......... .......... 11.1M\n   100K .....                                                  9.83T=0.01s\n\n2024-10-01 12:45:23 (8.25 MB/s) - ‘armaghdata.txt’ saved [107804]\n\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df = pd.read_csv(\n/tmp/ipykernel_2231/3811952515.py:41: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n  rain.loc[:\"2020\"].resample(\"Y\").mean().to_csv(\"rain.csv\", date_format=\"%Y\", index_label=False, float_format='%.1f')"
  },
  {
    "objectID": "answers/answer_filtering_party_size.html",
    "href": "answers/answer_filtering_party_size.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[tips[\"size\"] &gt;= 5]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n125\n29.80\n2.94\nThur\nLunch\n6\n\n\n141\n34.30\n4.69\nThur\nLunch\n6\n\n\n142\n41.19\n3.50\nThur\nLunch\n5\n\n\n143\n27.05\n3.50\nThur\nLunch\n6\n\n\n155\n29.85\n3.60\nSun\nDinner\n5\n\n\n156\n48.17\n3.50\nSun\nDinner\n6\n\n\n185\n20.69\n3.50\nSun\nDinner\n5\n\n\n187\n30.46\n1.40\nSun\nDinner\n5\n\n\n216\n28.15\n2.10\nSat\nDinner\n5"
  },
  {
    "objectID": "answers/answer_analysis_smallest_tip.html",
    "href": "answers/answer_analysis_smallest_tip.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nindex_of_smallest_bill = tips[\"total_bill\"].idxmin()\ntips[\"tip\"][index_of_smallest_bill]\n\n0.7"
  },
  {
    "objectID": "answers/answer_plot_annotate.html",
    "href": "answers/answer_plot_annotate.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\nfrom pandas import Series\n\n\nimport matplotlib.pyplot as plt\n\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/cetml1659on.txt\",  # file name\n    skiprows=6,  # skip header\n    delim_whitespace=True,  # whitespace separated\n    na_values=['-99.9', '-99.99'],  # NaNs\n)\n\n/tmp/ipykernel_2418/3659641096.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  temperature = pd.read_csv(\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 temperature = pd.read_csv(\n      2     \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/cetml1659on.txt\",  # file name\n      3     skiprows=6,  # skip header\n      4     delim_whitespace=True,  # whitespace separated\n      5     na_values=['-99.9', '-99.99'],  # NaNs\n      6 )\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\n\nfig, ax = plt.subplots()\n\ntemperature['JAN'].plot(color=\"steelblue\", ax=ax)\ntemperature['JUN'].plot(color=\"firebrick\", ax=ax)\ntemperature['YEAR'].plot(color=\"green\", linestyle=\"--\", ax=ax)\n\nax.set_xlabel(r'Year')\nax.set_ylabel(r'Temperature ($^\\circ$C)')\nax.legend(loc='upper left')\n\nwarm_winter_year = temperature['JAN'].idxmax()\nwarm_winter_temp = temperature['JAN'].max()\n\nax.annotate('Warmest winter',\n            xy=(warm_winter_year, warm_winter_temp), xycoords='data',\n            xytext=(-100, +30), textcoords='offset points', fontsize=12,\n            arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3,rad=-.2\"))\n\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 3\n      1 fig, ax = plt.subplots()\n----&gt; 3 temperature['JAN'].plot(color=\"steelblue\", ax=ax)\n      4 temperature['JUN'].plot(color=\"firebrick\", ax=ax)\n      5 temperature['YEAR'].plot(color=\"green\", linestyle=\"--\", ax=ax)\n\nNameError: name 'temperature' is not defined"
  },
  {
    "objectID": "answers/answer_plot_colours.html",
    "href": "answers/answer_plot_colours.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\nfrom pandas import Series\n\n\nimport matplotlib.pyplot as plt\n\n\ntemperature = pd.read_csv(\n    \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/cetml1659on.txt\",  # file name\n    skiprows=6,  # skip header\n    delim_whitespace=True,  # whitespace separated\n    na_values=['-99.9', '-99.99'],  # NaNs\n)\n\n/tmp/ipykernel_2479/3659641096.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  temperature = pd.read_csv(\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 temperature = pd.read_csv(\n      2     \"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/cetml1659on.txt\",  # file name\n      3     skiprows=6,  # skip header\n      4     delim_whitespace=True,  # whitespace separated\n      5     na_values=['-99.9', '-99.99'],  # NaNs\n      6 )\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nTaking the colours from the matplotlib docs:\n\nyear_plot = temperature['JAN'].plot(color=\"steelblue\")\nyear_plot = temperature['JUN'].plot(color=\"firebrick\")\n\nyear_plot.set_xlabel(r'Year')\nyear_plot.set_ylabel(r'Temperature ($^\\circ$C)')\n\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 year_plot = temperature['JAN'].plot(color=\"steelblue\")\n      2 year_plot = temperature['JUN'].plot(color=\"firebrick\")\n      4 year_plot.set_xlabel(r'Year')\n\nNameError: name 'temperature' is not defined\n\n\n\nAnd taking the line style from the docs again:\n\nyear_plot = temperature['JAN'].plot(color=\"steelblue\")\nyear_plot = temperature['JUN'].plot(color=\"firebrick\")\nyear_plot = temperature['YEAR'].plot(color=\"green\", linestyle=\"--\")\n\nyear_plot.set_xlabel(r'Year')\nyear_plot.set_ylabel(r'Temperature ($^\\circ$C)')\n\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 year_plot = temperature['JAN'].plot(color=\"steelblue\")\n      2 year_plot = temperature['JUN'].plot(color=\"firebrick\")\n      3 year_plot = temperature['YEAR'].plot(color=\"green\", linestyle=\"--\")\n\nNameError: name 'temperature' is not defined"
  },
  {
    "objectID": "answers/answer_visualisation_relplot.html",
    "href": "answers/answer_visualisation_relplot.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\ntips[\"percent_tip\"] = (tips[\"tip\"] / tips[\"total_bill\"])*100\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 tips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\n      2 tips[\"percent_tip\"] = (tips[\"tip\"] / tips[\"total_bill\"])*100\n\nNameError: name 'tips' is not defined\n\n\n\n\nimport seaborn as sns\n\nsns.relplot(\n    data=tips,\n    x=\"bill_per_person\",\n    y=\"percent_tip\",\n    hue=\"day\",\n    style=\"day\",\n).set(\n    xlabel=\"Bill per person (£)\",\n    ylabel=\"Tip percent\",\n)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 4\n      1 import seaborn as sns\n      3 sns.relplot(\n----&gt; 4     data=tips,\n      5     x=\"bill_per_person\",\n      6     y=\"percent_tip\",\n      7     hue=\"day\",\n      8     style=\"day\",\n      9 ).set(\n     10     xlabel=\"Bill per person (£)\",\n     11     ylabel=\"Tip percent\",\n     12 )\n\nNameError: name 'tips' is not defined"
  },
  {
    "objectID": "answers/answer_analysis_get_column.html",
    "href": "answers/answer_analysis_get_column.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"size\"]\n\n0      2\n1      3\n2      3\n3      2\n4      4\n      ..\n239    3\n240    2\n241    2\n242    2\n243    2\nName: size, Length: 244, dtype: int64"
  },
  {
    "objectID": "answers/answer_analysis_per_person_column.html",
    "href": "answers/answer_analysis_per_person_column.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "import pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\nbill_per_person\n\n\n\n\n0\n16.99\n0.71\nSun\nDinner\n2\n8.495000\n\n\n1\n10.34\n1.16\nSun\nDinner\n3\n3.446667\n\n\n2\n21.01\n2.45\nSun\nDinner\n3\n7.003333\n\n\n3\n23.68\n2.32\nSun\nDinner\n2\n11.840000\n\n\n4\n24.59\n2.53\nSun\nDinner\n4\n6.147500\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n9.676667\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n13.590000\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n11.335000\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n8.910000\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n9.390000\n\n\n\n\n244 rows × 6 columns"
  },
  {
    "objectID": "pages/01 Pandas.html",
    "href": "pages/01 Pandas.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "Pandas is a Python library providing high-performance, easy-to-use data structures and data analysis tools. Pandas provides easy and powerful ways to import data from a variety of sources and export it to just as many. It is also explicitly designed to handle missing data elegantly which is a very common problem in data from the real world.\nIt can be used to perform the same tasks that you might use spreadsheets such as Excel for: columns of data being combined together with functions being applied to them and finally being displayed as a graph.\nThe official pandas documentation is very comprehensive and you will be able to answer a lot of questions in there, however, it can sometimes be hard to find the right page. Don’t be afraid to use Google to find help.\nMost data analyses will follow a similar series of steps which we will go through in this course:\n\n    \n        \n            The first step of any data analysis is getting hold of some data.\n            This may be data you've collected yourself or perhaps from some external source.\n        \n        \n            Pandas provides you with the tools you need to read in a variety of different formats and to deal with the data cleaning that inevitably will be required in the real world.\n        \n    \n    \n    \n    \n        Pandas provides us with all the tools we need to be able to select, query, filter and combine our data.\n        We will start with the basics of selcting our data and then move on to more complex selections and how to ask questions of your data.\n    \n    \n    \n        \n            Working with raw data will only get you so far.\n            At the end we will learn how we can visualise our data with graphs and plot to communicate our results best.",
    "crumbs": [
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "pages/04 Filtering.html",
    "href": "pages/04 Filtering.html",
    "title": "Filtering data",
    "section": "",
    "text": "In the last section we looked at how to act on entire columns at once. For example when we did:\ntips[\"total_bill\"] * 100\nit applied the multiplication to every row, multiplying each number by 100.\nSometimes we don’t want to have to deal with entire columns at once, we might only want to grab a subset of the data and look in just that part. For example, with the tips data, we might think that the day of the week will affect the data so we just want to grab the data for Saturdays.\nIn Pandas there are two steps to asking a question like this.\n\ncreate a filter which describes the question you want to ask\napply that filter to the data to get just the bits you are interested in\n\nYou create a filter by performing some operation on your DataFrame or a column within it. To ask about only those rows which refer to Saturday, you grab the day column and compare it to \"Sat\":\nimport pandas as pd\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\ntips[\"day\"] == \"Sat\"\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n239     True\n240     True\n241     True\n242     True\n243    False\nName: day, Length: 244, dtype: bool\n\n\nThis has created a filter object (sometimes called a mask or a boolean array) which has True set for the rows where the day is Saturday and False elsewhere.\nWe could save this filter as a variable:\n\nsat_filter = tips[\"day\"] == \"Sat\"\n\nWe can use this to filter the DataFrame as a whole. tips[\"day\"] == \"Sat\" has returned a Series containing booleans. Passing it back into tips as an indexing operation will use it to filter based on the day column, only keeping those rows which contained True in the filter:\n\ntips[sat_filter]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n19\n20.65\n2.34\nSat\nDinner\n3\n\n\n20\n17.92\n2.86\nSat\nDinner\n2\n\n\n21\n20.29\n1.92\nSat\nDinner\n2\n\n\n22\n15.77\n1.56\nSat\nDinner\n2\n\n\n23\n39.42\n5.31\nSat\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n\n\n238\n35.83\n3.27\nSat\nDinner\n3\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n\n\n\n\n87 rows × 5 columns\n\n\n\nNotice that it now says that the table only has 87 rows, down from 244. However, the index has been maintained. This is because the row labels are connected to the row, they’re not just row numbers.\nIt is more common to do this in one step, rather than creating and naming a filter object. So the code becomes:\n\ntips[tips[\"day\"] == \"Sat\"]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n19\n20.65\n2.34\nSat\nDinner\n3\n\n\n20\n17.92\n2.86\nSat\nDinner\n2\n\n\n21\n20.29\n1.92\nSat\nDinner\n2\n\n\n22\n15.77\n1.56\nSat\nDinner\n2\n\n\n23\n39.42\n5.31\nSat\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n\n\n238\n35.83\n3.27\nSat\nDinner\n3\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n\n\n\n\n87 rows × 5 columns\n\n\n\nThis has given us back our subset of data as another DataFrame which can used in exactly the same way as the previous one (further filtering, summarising etc.).\n\n\n\n\n\n\nExercise\n\n\n\n\nSelect the data for only Thursdays.\nCalculate the mean of the tip column for Thursdays\nCompare this with the mean of the tip column for Saturdays\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nthurs = tips[tips[\"day\"] == \"Thur\"]\nthurs\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n77\n27.20\n2.80\nThur\nLunch\n4\n\n\n78\n22.76\n2.10\nThur\nLunch\n2\n\n\n79\n17.29\n1.90\nThur\nLunch\n2\n\n\n80\n19.44\n2.10\nThur\nLunch\n2\n\n\n81\n16.66\n2.38\nThur\nLunch\n2\n\n\n...\n...\n...\n...\n...\n...\n\n\n202\n13.00\n1.40\nThur\nLunch\n2\n\n\n203\n16.40\n1.75\nThur\nLunch\n2\n\n\n204\n20.53\n2.80\nThur\nLunch\n4\n\n\n205\n16.47\n2.26\nThur\nLunch\n3\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n\n\n\n\n62 rows × 5 columns\n\n\n\n\nthurs[\"tip\"].mean()\n\n1.9398387096774192\n\n\n\ntips[tips[\"day\"] == \"Sat\"][\"tip\"].mean()\n\n2.095402298850575\n\n\n\n\n\n\nOther filters\nAs well as filtering with the == operator (which only checks for exact matches), you can do other types of comparisons. Any of the standard Python comparisons will work (i.e. ==, !=, &lt;, &lt;=, &gt;, &gt;=).\nTo grab only the rows where the total bill is less than £8 we can use &lt;:\n\ntips[tips[\"total_bill\"] &lt; 8]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n67\n3.07\n0.70\nSat\nDinner\n1\n\n\n92\n5.75\n0.70\nFri\nDinner\n2\n\n\n111\n7.25\n0.70\nSat\nDinner\n1\n\n\n149\n7.51\n1.40\nThur\nLunch\n2\n\n\n172\n7.25\n3.60\nSun\nDinner\n2\n\n\n195\n7.56\n1.01\nThur\nLunch\n2\n\n\n218\n7.74\n1.01\nSat\nDinner\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFilter the data to only include parties of 5 or more people.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[tips[\"size\"] &gt;= 5]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n125\n29.80\n2.94\nThur\nLunch\n6\n\n\n141\n34.30\n4.69\nThur\nLunch\n6\n\n\n142\n41.19\n3.50\nThur\nLunch\n5\n\n\n143\n27.05\n3.50\nThur\nLunch\n6\n\n\n155\n29.85\n3.60\nSun\nDinner\n5\n\n\n156\n48.17\n3.50\nSun\nDinner\n6\n\n\n185\n20.69\n3.50\nSun\nDinner\n5\n\n\n187\n30.46\n1.40\nSun\nDinner\n5\n\n\n216\n28.15\n2.10\nSat\nDinner\n5\n\n\n\n\n\n\n\n\n\n\n\n\nCombining filters\nIf you want to apply multiple filters, for example to select only “Saturdays with small total bills” you can do it in one of two different ways. Either split the question into multiple steps, or ask it all at once.\nLet’s do it multiple steps first since we already have tools we need for that:\n\nsat_tips = tips[tips[\"day\"] == \"Sat\"]  # First grab the Saturday data and save it as a variable\nsat_tips[sat_tips[\"total_bill\"] &lt; 8]  # Then act on the new DataFrame as use it as before\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n67\n3.07\n0.70\nSat\nDinner\n1\n\n\n111\n7.25\n0.70\nSat\nDinner\n1\n\n\n218\n7.74\n1.01\nSat\nDinner\n2\n\n\n\n\n\n\n\nOr, you can combine the questions together using the & operator with a syntax like:\ndf[(filter_1) & (filter_2)]\nso in our case filter 1 is tips[\"day\"] == \"Sat\" and filter 2 is tips[\"total_bill\"] &lt; 8 so it becomes:\n\ntips[(tips[\"day\"] == \"Sat\") & (tips[\"total_bill\"] &lt; 8)]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n67\n3.07\n0.70\nSat\nDinner\n1\n\n\n111\n7.25\n0.70\nSat\nDinner\n1\n\n\n218\n7.74\n1.01\nSat\nDinner\n2\n\n\n\n\n\n\n\nIf you want to do an “or” operation, then instead of & you can use |.\n\n\n\n\n\n\nExercise 3\n\n\n\nFilter the data to only include parties of 4 or more people which happened at lunch time.\nHint: The size and time columns are what you want to use here.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[(tips[\"size\"] &gt;= 4) & (tips[\"time\"] == \"Lunch\")]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n77\n27.20\n2.80\nThur\nLunch\n4\n\n\n85\n34.83\n3.62\nThur\nLunch\n4\n\n\n119\n24.08\n2.04\nThur\nLunch\n4\n\n\n125\n29.80\n2.94\nThur\nLunch\n6\n\n\n141\n34.30\n4.69\nThur\nLunch\n6\n\n\n142\n41.19\n3.50\nThur\nLunch\n5\n\n\n143\n27.05\n3.50\nThur\nLunch\n6\n\n\n197\n43.11\n3.50\nThur\nLunch\n4\n\n\n204\n20.53\n2.80\nThur\nLunch\n4\n\n\n\n\n\n\n\n\n\n\n\n\nDataFrame indexing\nWhen we use the square bracket syntax on a DataFrame directly there are a few different types of object that can be passed:\n\n\nA single string\n\n\nThis will select a single column form the DataFrame, returning a Series object.\n\n\nA list of strings\n\n\nThis will select those columns by name, returning a DataFrame.\n\n\nA filter (a Series of True/False)\n\n\nThis will filter the table as a whole, returning a DataFrame with only the rows matching Trueincluded.\n\n\nThese are provided as shortcuts as they are the most common operations to do an a DataFrame. This is why some of them operate on columns and other on rows.\nIf you want to be explicit about which axis you are acting on, you can pass these same types of objects to the .loc[rows, columns] attribute with one argument per axis. This means that\ntips[sat_filter]\nis equivalent to\ntips.loc[sat_filter]\nand that\ntips[\"size\"]\nis equivalent to\ntips.loc[:, \"size\"]\nThe full set of rules for DataFrame.loc are in the documentation.",
    "crumbs": [
      "Filtering data"
    ]
  },
  {
    "objectID": "pages/99 Contributors.html",
    "href": "pages/99 Contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "This course was originally written by Matt Williams, see https://milliams.com/courses/intermediate_python/.\nThe course has since been modified by the Jean Golding Institute.\n\nChristopher Woods\nMatt Williams\nLéo Gorman\nJames Thomas\nPau Erola",
    "crumbs": [
      "Contributors"
    ]
  },
  {
    "objectID": "pages/03 Analysis.html",
    "href": "pages/03 Analysis.html",
    "title": "Querying your data",
    "section": "",
    "text": "Once your data is read in and available as a DataFrame, Pandas provides a whole suite of tools for extracting information from it.\nLet’s start by looking at some example data which contains information about the amounts that people at a restaurant paid and tipped for their meals:\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\ntips\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\n\n\n\n\n0\n16.99\n0.71\nSun\nDinner\n2\n\n\n1\n10.34\n1.16\nSun\nDinner\n3\n\n\n2\n21.01\n2.45\nSun\nDinner\n3\n\n\n3\n23.68\n2.32\nSun\nDinner\n2\n\n\n4\n24.59\n2.53\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n\n\n\n\n244 rows × 5 columns\n\n\n\nThe first thing that you usually want to be able to do is to pull apart the overall table to get at specific bits of data from inside.\nWhen using lists and dicts in Python, the square-bracket syntax was used to fetch an item from the container. In Pandas we can use the same syntax but it’s a much more powerful tool.\nIf you pass a single string to the square brackets of a DataFrame it will return to you just that one column:\n\ntips[\"total_bill\"]\n\n0      16.99\n1      10.34\n2      21.01\n3      23.68\n4      24.59\n       ...  \n239    29.03\n240    27.18\n241    22.67\n242    17.82\n243    18.78\nName: total_bill, Length: 244, dtype: float64\n\n\nAccessing a column like this returns an object called a Series which is the second of the two main Pandas data types. Don’t worry too much about these just yet but think of them as being a single column of the DataFrame, along with the index of the DataFrame.\nIf you pass a list of column names to the square brackets then you can grab out just those columns:\n\ntips[[\"total_bill\", \"tip\"]]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\n\n\n\n\n0\n16.99\n0.71\n\n\n1\n10.34\n1.16\n\n\n2\n21.01\n2.45\n\n\n3\n23.68\n2.32\n\n\n4\n24.59\n2.53\n\n\n...\n...\n...\n\n\n239\n29.03\n4.14\n\n\n240\n27.18\n1.40\n\n\n241\n22.67\n1.40\n\n\n242\n17.82\n1.22\n\n\n243\n18.78\n2.10\n\n\n\n\n244 rows × 2 columns\n\n\n\n\n\nAside: Indexing\nNote that the outer square brackets are saying “I’m selecting data” and the inner square brackets are saying “I’m giving you a list of column names”.\nIn this case it gives you back another DataFrame, just with only the required columns present.\n\nGetting rows\nIf you want to select a row from a DataFrame then you can use the .loc (short for “location”) attribute which allows you to pass index values like:\n\ntips.loc[2]\n\ntotal_bill     21.01\ntip             2.45\nday              Sun\ntime          Dinner\nsize               3\nName: 2, dtype: object\n\n\nIf you want to grab a single value from the table, you can follow the row label with the column name that you want:\n\ntips.loc[2, \"total_bill\"]\n\n21.01\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe size column in the data is the number of people in the dining party. Extract this column from the DataFrame.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"size\"]\n\n0      2\n1      3\n2      3\n3      2\n4      4\n      ..\n239    3\n240    2\n241    2\n242    2\n243    2\nName: size, Length: 244, dtype: int64\n\n\n\n\n\n\n\nDescriptive statistics\nNow that we know how to refer to individual columns, we can start asking questions about the data therein. If you’ve worked with columns of data in Excel for example, you’ve probably come across the SUM() and AVERAGE() functions to summarise data. We can do the same thing in pandas by calling the sum() or mean() methods on a column:\n\ntips[\"total_bill\"].sum()\n\n4827.77\n\n\n\ntips[\"total_bill\"].mean()\n\n19.78594262295082\n\n\nYou can see a list of all the possible functions you can call in the documentation for Series. So for example, you can also ask for the maximum value from a column with the max() method.\n\ntips[\"tip\"].max()\n\n7.0\n\n\nIn some situations, you don’t just want to get the value of the maximum, but rather to find out which row it came from. In cases like that there is the idxmax() method which give you the index label of the row with the maximum:\n\ntips[\"total_bill\"].idxmax()\n\n170\n\n\nSo we know that the value of the maximum bill was £7 and it was found in the row with the label 170.\nYou can then use this information with the .loc attribute to get the rest of the information for that row:\n\nindex_of_max_bill = tips[\"total_bill\"].idxmax()\ntips.loc[index_of_max_bill]\n\ntotal_bill     50.81\ntip              7.0\nday              Sat\ntime          Dinner\nsize               3\nName: 170, dtype: object\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFind the value of the tip that was paid for the smallest total bill.\nHint: Have a look at the documentation page for Series. There’s a function which works like idxmax() but finds the minimum.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nindex_of_smallest_bill = tips[\"total_bill\"].idxmin()\ntips[\"tip\"][index_of_smallest_bill]\n\n0.7\n\n\n\n\n\n\n\nActing on columns\nFunctions like sum() and max() summarise down the column to a single value. In some situations we instead want to manipulate a column to create a new column.\nFor example, the data in the table is in British pounds. If we wanted to convert it into the number of pennies then we need to multiply each value by 100. In Pandas you can refer to an entire column and perform mathematical operations on it and it will apply the operation to each row:\n\ntips[\"total_bill\"] * 100\n\n0      1699.0\n1      1034.0\n2      2101.0\n3      2368.0\n4      2459.0\n        ...  \n239    2903.0\n240    2718.0\n241    2267.0\n242    1782.0\n243    1878.0\nName: total_bill, Length: 244, dtype: float64\n\n\nThe data in row 0 was previously 16.99 but the result here is 1699.0, and likewise for every other row.\nYou can do any mathematical operation that Python supports, such as +, - and /.\n\n\nCombining columns\n\n\nAside\nColumns are actually combined by matching together their index labels, not strictly by their position in the column.\nAs well as operating on individual columns, you can combine together multiple columns. Any operation you do between two columns will be done row-wise, that is adding two columns will add together the two values from the first row of each, then the second row from each etc.\nFor example if we wanted to find out, for each entry in our table what the ratio between tip amount and total bill was, we could divide one column by the other:\n\ntips[\"tip\"] / tips[\"total_bill\"]\n\n0      0.041789\n1      0.112186\n2      0.116611\n3      0.097973\n4      0.102887\n         ...   \n239    0.142611\n240    0.051508\n241    0.061756\n242    0.068462\n243    0.111821\nLength: 244, dtype: float64\n\n\nOf course, if we want the tip percentage so we need to multiply the value by 100:\n\n(tips[\"tip\"] / tips[\"total_bill\"])*100\n\n0       4.178929\n1      11.218569\n2      11.661114\n3       9.797297\n4      10.288735\n         ...    \n239    14.261109\n240     5.150846\n241     6.175562\n242     6.846240\n243    11.182109\nLength: 244, dtype: float64\n\n\nIt can get messy and hard-to-read doing too many things on one line, so it’s a good idea to split each part of your calculation onto its own line, giving each step its own variable name along the way.\n\ntip_fraction = tips[\"tip\"] / tips[\"total_bill\"]\ntip_percent = tip_fraction*100\ntip_percent\n\n0       4.178929\n1      11.218569\n2      11.661114\n3       9.797297\n4      10.288735\n         ...    \n239    14.261109\n240     5.150846\n241     6.175562\n242     6.846240\n243    11.182109\nLength: 244, dtype: float64\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe total_bill column give the total amount for the entire dining party. Calculate the amount spent per person for each row in the DataFrame.\nExtra: calculate the average and the standard deviation of this data. You might need to take a look at the documentation page for the Series type.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\nbill_per_person = tips[\"total_bill\"] / tips[\"size\"]\nbill_per_person\n\n0       8.495000\n1       3.446667\n2       7.003333\n3      11.840000\n4       6.147500\n         ...    \n239     9.676667\n240    13.590000\n241    11.335000\n242     8.910000\n243     9.390000\nLength: 244, dtype: float64\n\n\n\nbill_per_person.mean()\n\n7.888229508196722\n\n\n\nbill_per_person.std()\n\n2.9143496626221\n\n\n\n\n\n\n\nAdding new columns\nNew columns can be added to a DataFrame by assigning them by index (as you would for a Python dict):\n\ntips[\"percent_tip\"] = (tips[\"tip\"] / tips[\"total_bill\"])*100\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\npercent_tip\n\n\n\n\n0\n16.99\n0.71\nSun\nDinner\n2\n4.178929\n\n\n1\n10.34\n1.16\nSun\nDinner\n3\n11.218569\n\n\n2\n21.01\n2.45\nSun\nDinner\n3\n11.661114\n\n\n3\n23.68\n2.32\nSun\nDinner\n2\n9.797297\n\n\n4\n24.59\n2.53\nSun\nDinner\n4\n10.288735\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n14.261109\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n5.150846\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n6.175562\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n6.846240\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n11.182109\n\n\n\n\n244 rows × 6 columns\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake the “bill per person” result you calculated in the last exercise and add it as a new column, bill_per_person, in the DataFrame.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport pandas as pd\n\ntips = pd.read_csv(\"https://bristol-training.github.io/introduction-to-data-analysis-in-python/data/tips.csv\")\n\n\ntips[\"bill_per_person\"] = tips[\"total_bill\"] / tips[\"size\"]\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nday\ntime\nsize\nbill_per_person\n\n\n\n\n0\n16.99\n0.71\nSun\nDinner\n2\n8.495000\n\n\n1\n10.34\n1.16\nSun\nDinner\n3\n3.446667\n\n\n2\n21.01\n2.45\nSun\nDinner\n3\n7.003333\n\n\n3\n23.68\n2.32\nSun\nDinner\n2\n11.840000\n\n\n4\n24.59\n2.53\nSun\nDinner\n4\n6.147500\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n4.14\nSat\nDinner\n3\n9.676667\n\n\n240\n27.18\n1.40\nSat\nDinner\n2\n13.590000\n\n\n241\n22.67\n1.40\nSat\nDinner\n2\n11.335000\n\n\n242\n17.82\n1.22\nSat\nDinner\n2\n8.910000\n\n\n243\n18.78\n2.10\nThur\nDinner\n2\n9.390000\n\n\n\n\n244 rows × 6 columns",
    "crumbs": [
      "Querying your data"
    ]
  }
]